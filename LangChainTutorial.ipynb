{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LangChain basics\n",
    "\n",
    "You can use prompt templates to insert data into your prompt easily. Don't forget to create environment variables for your OpenAI or Azure OpenAI keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Colorful Cozies.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Using completions API\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "\n",
    "# Declare our prompt template.\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "# Dynamically pass in our params.\n",
    "llm_result = llm(prompt.format(product=\"colorful socks\"))\n",
    "\n",
    "print(llm_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LangChain to get structured data outputs\n",
    "\n",
    "You can use LangChain to get JSON responses. Below I am wanting my question and answer in a structured format. But you can build whatever models you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QnA(question='What is a good name for a company that makes colorful socks?', answer='Socktastic!')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator\n",
    "\n",
    "# I am chosing Q&A here, but really you can chose any structure you need for your class. \n",
    "# It knows how to generate the class structure based on what you put in description - magic?\n",
    "class QnA(BaseModel):\n",
    "    question: str = Field(description=\"question\")\n",
    "    answer: str = Field(description=\"answer\")\n",
    "        \n",
    "actor_query = \"What is a good name for a company that makes colorful socks?\"\n",
    "\n",
    "# You can use different parsers, or even construct your own. Pydantic creates nice objects for python so I am using that.\n",
    "parser = PydanticOutputParser(pydantic_object=QnA)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "_input = prompt.format_prompt(query=actor_query)\n",
    "\n",
    "output = llm(_input.to_string())\n",
    "\n",
    "parser.parse(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following demonstrates how to use Conversation History and Memory in a conversation chain\n",
    "\n",
    "LangChain can help remember what the user is saying for back and forth chatgpt like capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! It's nice to meet you. How can I help you today?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# The conversation object manages the back and forward conversation, including memory.\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "# We just add user input and let the conversation object handle the rest.\n",
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That's great! It's always nice to have a conversation with someone new. What would you like to talk about?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding more user input to the conversation chain keeps the memory, see the verbose output below.\n",
    "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Tools\n",
    "\n",
    "You can set custom tools to be used, for example here is the bing search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'About. I help companies transform into data driven organisations so they can become predictive instead of reactive. I’ve created a ‘data adoption framework’ that brings organisation from strategy...'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"BING_SUBSCRIPTION_KEY\"] = \"\"\n",
    "os.environ[\"BING_SEARCH_URL\"] = \"\"\n",
    "\n",
    "from langchain.utilities import BingSearchAPIWrapper\n",
    "search = BingSearchAPIWrapper(k=1)\n",
    "search.run(\"Dave Enright from Singapore on LinkedIn\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can use agents to orchestrate multiple tools together\n",
    "\n",
    "Below the agent is being told to use the Bing Search tool when it needs to answer current events.The agent will use this tool based on the kinds of questions the user is asking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, AgentExecutor, BaseSingleActionAgent\n",
    "from langchain.utilities import BingSearchAPIWrapper\n",
    "\n",
    "# The tool description helps langchain work out when to use the tool. Be careful with your descriptions as its not completely deterministic.\n",
    "search = BingSearchAPIWrapper(k=1)\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"Intermediate Answer\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events\",\n",
    "        return_direct=True\n",
    "    )\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising the agent to use the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Yes.\n",
      "Follow up: What is the population of Thailand in 2020?\u001b[0m\n",
      "Intermediate answer: \u001b[36;1m\u001b[1;3mThe current <b>population</b> <b>of Thailand</b> is 70,301,299 as of Wednesday, May 17, 2023, based on ...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current <b>population</b> <b>of Thailand</b> is 70,301,299 as of Wednesday, May 17, 2023, based on ...'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Tuple, Any, Union\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "# There are different kinds of agents documented on the langchain website. This particular one is basically just a chat with search agent.\n",
    "self_ask_with_search = initialize_agent(tools, llm, agent=AgentType.SELF_ASK_WITH_SEARCH, verbose=True)\n",
    "self_ask_with_search.run(\"How many people live in Thailand in 2023?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# There are different document loaders available, this is the basic PDF loader.\n",
    "loader = PyPDFLoader(\"Benefit_Options.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='This document contains information generated using a language model (Azure OpenAI). The information \\ncontained in this document is only for demonstration purposes and does not reflect the opinions or \\nbeliefs of Microsoft. Microsoft makes no representations or warranties of any kind, express or implied, \\nabout the completeness, accuracy, reliability, suitability or availability with respect to the information \\ncontained in this document.  \\nAll rights reserved to Microsoft', metadata={'source': 'Benefit_Options.pdf', 'page': 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting into pages is useful to give page number back as a source\n",
    "pages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3: offers a wider range of prescription drug coverage than Northwind Standard. Both plans offer coverage \n",
      "for vision and dental services, as well as medical services.  \n",
      "Next Steps  \n",
      "We hope that this information has been helpful in understanding the differe nces between Northwind \n",
      "Health Plus and North\n",
      "2: Welcome to Contoso  Electronics ! We are excited to offer our employees two comprehensive health \n",
      "insurance plans through Northwind Health.  \n",
      "Northwind Health Plus  \n",
      "Northwind Health Plus is a comprehensive plan that provides comprehensive coverage for medical, \n",
      "vision, and dental services. T his pl\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# This is creating an index based on embeddings, but as you can see by the output it's a bit ugly to work with.\n",
    "faiss_index = FAISS.from_documents(pages, OpenAIEmbeddings())\n",
    "docs = faiss_index.similarity_search(\"What's the difference between plus and standard?\", k=2)\n",
    "for doc in docs:\n",
    "    print(str(doc.metadata[\"page\"]) + \":\", doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "# Using the vector store from the loader means langchain can abstract away a lot of the under the hood stuff.\n",
    "# Note I am using a local python package for the vectorDB. You may get errors.\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"What's the difference between healthcare plans?\",\n",
       " 'answer': ' Northwind Health Plus offers more comprehensive coverage than Northwind Standard, including coverage for emergency services, mental health and substance abuse, and out-of-network services. Northwind Standard does not offer coverage for emergency services, mental health and substance abuse coverage, or out-of-network services.\\n',\n",
       " 'sources': 'Benefit_Options.pdf'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It comes up with better formatted responses.\n",
    "query = \"What's the difference between healthcare plans?\"\n",
    "index.query_with_sources(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
